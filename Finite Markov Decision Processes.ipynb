{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes\n",
    "\n",
    "> Guided by [Sutton, R. and Barto, A. (2018). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press, pp.47-71. Chapter 3.](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "\n",
    "The problem of finite MDPs involves evaluative feedback and choosing different actions in different situations.\n",
    "\n",
    "Actions influence not only immediate rewards, but also subsequent situations or states, and through those future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The agent-environment interface\n",
    "\n",
    "If the state includes information about all aspects of the past agent-environment interaction that make a difference for the future, then the state is said to have the Markov property.\n",
    "\n",
    "The agent-environment boundary can be located at different places for different purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3.1* Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3.2* Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3.3* Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3.4* Give a table analogous to that in Example 3.3, but for $p(s',r|s,a)$. It should have columns for $s$, $a$, $s'$, $r$, and $p(s',r|s,a)$, and a row for every 4-tuple for which $p(s',r|s,a) > 0$.\n",
    "\n",
    "| $s$  | $a$      | $s'$ | $r$               | $p(s',r|s,a)$       |\n",
    "|------|----------|------|-------------------|---------------------|\n",
    "| high | search   | high | $r_\\text{search}$ | $\\alpha$            |\n",
    "| high | search   | low  | $r_\\text{search}$ | $1 - \\alpha$        |\n",
    "| high | wait     | high | $r_\\text{wait}$   | 1                   |\n",
    "| low  | search   | low  | $r_\\text{search}$ | $\\beta$             |\n",
    "| low  | search   | high | -3                | $1 - \\beta$         |\n",
    "| low  | wait     | low  | $r_\\text{wait}$   | 1                   |\n",
    "| low  | recharge | high | 0                 | 1 (text for format) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Rewards\n",
    "\n",
    "The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns and Episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3.5* The equations in Section 3.1 the agent-environment interface are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of (3.3).\n",
    "\n",
    "3.3: $\\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a) = 1$, for all $s\\in S$, $a\\in A(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.4: Pole-Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3.6* Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for 1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?\n",
    "\n",
    "- episodic using discounting\n",
    "\n",
    "  $$\n",
    "  \\begin{align}\n",
    "  G_t &= R_{t+1} + rR_{t+2} + r^2R_{t+3} + \\dots + r^{K-1}R_K + r^KR_{K+1}\\\\\n",
    "      &= r^K(-1)\\\\\n",
    "      &= -r^K\\\\\n",
    "  \\end{align}\n",
    "  $$\n",
    "  \n",
    "- continuing using discounting\n",
    "\n",
    "  $$\n",
    "  \\begin{align}\n",
    "  G_t &= R_{t+1} + rR_{t+2} + r^2R_{t+3} + \\dots + r^{K_1-1}R_{k_1} + r^{K_1}R_{K_1+1} + r^{K_1+1}R_{K_1+2} + \\dots + r^{K_2-1}R_{k_2} + r^{K_2}R_{K_2+1} + r^{K_2+1}R_{K_2+2} + \\dots\\\\\n",
    "      &= r^{K_1}(-1) + r^{K_2}(-1) + \\dots\\\\\n",
    "      &= -r^{K_1} - r^{K_2} + \\dots\\\\\n",
    "      &= -\\sum^\\infty_{k=1} r^{K_k}\n",
    "  \\end{align}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3.7* Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward. After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n",
    "\n",
    "There is no rewards signal on the actions the robot take to solve the maze. It failed to communicate to the agent to take fewer steps to solve the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3.8* Suppose $\\gamma = 0.5$ and the following sequence of rewards is received $R_1 = 1$, $R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0$ ,$G_1$, $\\dots$, $G_5$? Hint: Work backwards.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\dots\\\\\n",
    "    &= R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\dots)\\\\\n",
    "    &= R_{t+1} + \\gamma G_{t+1}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $G_5 = 0$\n",
    "- $G_4 = R_5 + \\gamma G_5 = 2$\n",
    "- $G_3 = R_4 + \\gamma G_4 = 3 + 0.5 \\times 2 = 4$\n",
    "- $G_2 = R_3 + \\gamma G_3 = 6 + 0.5 \\times 4 = 8$\n",
    "- $G_1 = R_2 + \\gamma G_2 = 2 + 0.5 \\times 8 = 6$\n",
    "- $G_0 = R_1 + \\gamma G_1 = -1 + 0.5 \\times 6 = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3.9* Suppose $\\gamma = 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite sequence of 7s. What are $G_1$ and $G_0$?\n",
    "\n",
    "- $$\n",
    "  \\begin{align}\n",
    "  G_1 &= R_2 + \\gamma R_3 + \\gamma^2 R_4 + \\gamma^3 R_5 + \\dots\\\\\n",
    "      &= 7 + \\gamma \\times 7 + \\gamma^3 \\times 7 + \\gamma^3 \\times 7 + \\dots\\\\\n",
    "      &= 7 (1 + \\gamma + \\gamma^2 + \\gamma^3 + \\dots)\\\\\n",
    "      &= 7 \\sum^\\infty_{k=0} \\gamma^k\\\\\n",
    "      &= 7 \\times \\frac{1}{1-\\gamma}\\\\\n",
    "      &= 7 \\times \\frac{1}{1-0.9}\\\\\n",
    "      &= 70\n",
    "  \\end{align}\n",
    "  $$\n",
    "- $$\n",
    "  \\begin{align}\n",
    "  G_0 &= R_1 + \\gamma G_1\\\\\n",
    "      &= 2 + 0.9 \\times 70\\\\\n",
    "      &= 65\n",
    "  \\end{align}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3.10* Prove the second equality in (3.10).\n",
    "\n",
    "3.10: $G_t = \\sum^\\infty_{k=0} \\gamma^k = \\frac{1}{1-\\gamma}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t &= \\sum^\\infty_{k=0} \\gamma^k\\\\\n",
    "G_t &= 1 + \\gamma + \\gamma^2 + \\gamma^3 + \\dots + \\gamma^{n-1}\\\\\n",
    "\\gamma G_t &= \\gamma + \\gamma^2 + \\gamma^3 + \\gamma^4 + \\dots + \\gamma^n\\\\\n",
    "G_t - \\gamma G_t &= 1 - \\gamma^n\\\\\n",
    "G_t(1-\\gamma) &= 1 - \\gamma^n\\\\\n",
    "G_t &= \\frac{1 - \\gamma^n}{1 - \\gamma}\\\\\n",
    "G_t &= \\frac{1}{1 - \\gamma}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
