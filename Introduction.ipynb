{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"People worry that computers will get too smart and take over the world, but the real problem is that they’re too stupid and they’ve already taken over the world.\" -- Pedro Domingos\n",
    "\n",
    "# Introduction\n",
    "\n",
    "> Guided by [Sutton, R. and Barto, A. (2018). *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, pp.1-22. Chapter 1](http://incompleteideas.net/book/the-book-2nd.html).\n",
    "\n",
    "Reinforcement learning has two characteristics: trial-and-error and delayed reward. Reinforcement learning is to learn what to do and learner discovers which actions yield the most reword by tryin them. Actions may affect not only the immediate reward but also subsequent rewards.\n",
    "\n",
    "The problem of reinforcement learning is formalized using idea from dynamical systems theory, as the optimal control of incompletely-known Markov decision processes (not worrying too much about this for now). Any methods that is well suited to solving such problems is considered to ba a reinforcement learning method.\n",
    "\n",
    "Three aspects of Markov decision processes: **sensation, action and goal**. The agent is able to sense the state to some extent and able to take actions affect the state. The agent also has a goal or goal relating to the state of the environment.\n",
    "\n",
    "## Compared with supervised learning\n",
    "\n",
    "Supervised learning learn from description of situation (input) with a specification (label). For the system to extrapolate or generalize.\n",
    "\n",
    "Reinforcement learning does not need examples of correct behaviour. The agent learns from its own experience. \n",
    "\n",
    "## Compared with unsupervised learning\n",
    "\n",
    "Unsupervised learning often to find structures. Uncoversin structure in an agent's experience can be useful but by itself does not address the reinforcement learning problem of maximizing a reward signal.\n",
    "\n",
    "## More about reinforcement learning\n",
    "\n",
    "There is a trade-off between exploration and exploitation.\n",
    "\n",
    "## Elements of reinforcement learning\n",
    "\n",
    "Four main subelements of a reinforcement learning system beyond the agent and the environment:\n",
    "\n",
    "- A *policy*\n",
    "\n",
    "  A mapping from states to action (what's the action when in that state).\n",
    "\n",
    "\n",
    "- A *reward signal*\n",
    "\n",
    "  Immediate.\n",
    "\n",
    "\n",
    "- A *value function*\n",
    "\n",
    "  Specifies what is good in the long run. The total amount of reward an agent can expect to accumulate over the future starting from that state.\n",
    "  \n",
    "  \n",
    "- A *model* of the environment (optional)\n",
    "\n",
    "  Allows inferences to be made about how the environment will behave. Can be used for planning.\n",
    "  \n",
    "## Other methods (not reinforcement learning)\n",
    "\n",
    "- Evolutionay methods can be effective if the space of policies is sufficiently small, or can be stractured so that good policies are common or easy to find, or if a lot of time is avaiable for the search. Evolutionary methods do not learn during each individual lifetime.\n",
    "\n",
    "- \"Minimax\" assumes a particular way of playin by the opponent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
